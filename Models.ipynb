{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siMU3H-PtdZr"
      },
      "source": [
        "\n",
        "\n",
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6u7Fpv8StdZu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import spearmanr\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lanxWKvItdZv"
      },
      "source": [
        "## Loading data\n",
        "\n",
        "- `X_train` and `X_test` both have $35$ columns that represent the same explanatory variables but over different time periods.\n",
        "\n",
        "- `X_train` and `Y_train` share the same column `ID` - each row corresponds to a unique ID associated wwith a day and a country.\n",
        "\n",
        "- The target of this challenge `TARGET` in `Y_train` corresponds to the price change for daily futures contracts of 24H electricity baseload.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsOj1In4tdZv"
      },
      "outputs": [],
      "source": [
        "X_train = pd.read_csv('X_train.csv')\n",
        "Y_train_clean = pd.read_csv('Y_train.csv')\n",
        "X_test = pd.read_csv('X_test.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5s3VIxytdZx"
      },
      "source": [
        "#Model and train score\n",
        "\n",
        "The benchmark for this challenge consists in a simple linear regression, after a light cleaning of the data: The missing (NaN) values are simply filled with 0's and the `COUNTRY` column is dropped - namely we used the same model for France and Germany."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def metric_train(output):\n",
        "\n",
        "    return  spearmanr(output, Y_train_clean).correlation"
      ],
      "metadata": {
        "id": "kCUzC4vXF9jB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Linear Regression"
      ],
      "metadata": {
        "id": "iV9_brn8KsKJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(Modified_X_train, Y_train_clean, test_size=0.2, random_state=42)\n",
        "\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, Y_train)\n",
        "\n",
        "output_train = lr.predict(X_train)\n",
        "output_val = lr.predict(X_val)\n",
        "\n",
        "mse = mean_squared_error(Y_val, output_val)\n",
        "print(\"MSE:\",mse)\n",
        "mae = mean_absolute_error(Y_val, output_val)\n",
        "print(\"MAE:\",mae)\n",
        "\n",
        "# Calculate Normalized Mean Absolute Error (NMAE)\n",
        "range_y_val = np.max(Y_val) - np.min(Y_val)\n",
        "nmae = mae / range_y_val\n",
        "print(\"NMAE:\", nmae)\n",
        "\n",
        "# Calculate Normalized Mean Squared Error (NMSE)\n",
        "var_y_val = np.var(Y_val)\n",
        "nmse = mse / var_y_val\n",
        "print(\"NMSE:\", nmse)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BWjFjTcGF6m5",
        "outputId": "f05434f9-0dc3-4b57-9f5b-efde3ea78115"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 1.1894424076353105\n",
            "MAE: 0.5889078075102198\n",
            "NMAE: 0.0517925434724655\n",
            "NMSE: 0.9958721897575211\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "cPddgaQDKn2B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train_clean, Y_train_clean, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest model with regularization parameters\n",
        "model = RandomForestRegressor(\n",
        "    n_estimators=200,   # Number of trees in the forest\n",
        "    max_depth=10,       # Maximum depth of the trees\n",
        "    min_samples_split=2, # Minimum number of samples required to split an internal node\n",
        "    min_samples_leaf=1,  # Minimum number of samples required to be at a leaf node\n",
        "    max_features='sqrt', # Number of features to consider when looking for the best split\n",
        "    bootstrap=True       # Whether bootstrap samples are used when building trees\n",
        ")\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "# Make predictions on the validation data\n",
        "output_val = model.predict(X_val)\n",
        "\n",
        "# Calculate the custom evaluation metric on the validation data\n",
        "spearman_correlation = metric_train(Y_val, output_val)\n",
        "print(\"Spearman correlation score on validation set:\", spearman_correlation)\n",
        "\n",
        "# Calculate mean squared error and mean absolute error\n",
        "mse = mean_squared_error(Y_val, output_val)\n",
        "print(\"MSE:\", mse)\n",
        "mae = mean_absolute_error(Y_val, output_val)\n",
        "print(\"MAE:\", mae)\n",
        "\n",
        "# Calculate Normalized Mean Absolute Error (NMAE)\n",
        "range_y_val = np.max(Y_val) - np.min(Y_val)\n",
        "nmae = mae / range_y_val\n",
        "print(\"NMAE:\", nmae)\n",
        "\n",
        "# Calculate Normalized Mean Squared Error (NMSE)\n",
        "var_y_val = np.var(Y_val)\n",
        "nmse = mse / var_y_val\n",
        "print(\"NMSE:\", nmse)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Zo1NwXEBzDB",
        "outputId": "25ddb868-e1b5-4213-d375-2670dfaf6174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman correlation score on validation set: 0.18289780251846197\n",
            "MSE: 1.2398234473032794\n",
            "MAE: 0.6052393187863744\n",
            "NMAE: 0.05322884724863274\n",
            "NMSE: 1.038054203762006\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze feature importance\n",
        "feature_importance = model.feature_importances_\n",
        "sorted_idx = np.argsort(feature_importance)[::-1]  # Sort feature indices by importance in descending order\n",
        "\n",
        "# Print feature importance\n",
        "print(\"Feature Importance:\")\n",
        "for i, idx in enumerate(sorted_idx):\n",
        "    print(f\"{i+1}. Feature {X_train.columns[idx]}: Importance = {feature_importance[idx]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slEIf-G1Dtds",
        "outputId": "34105e12-f446-49b2-9620-9faa8cc3929b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importance:\n",
            "1. Feature COUNTRY_FR: Importance = 0.05970705178117934\n",
            "2. Feature COUNTRY_DE: Importance = 0.05868735762669619\n",
            "3. Feature GAS_RET: Importance = 0.054989015837217665\n",
            "4. Feature FR_GAS: Importance = 0.047685020488853896\n",
            "5. Feature CARBON_RET: Importance = 0.04470285986916853\n",
            "6. Feature FR_WINDPOW: Importance = 0.04343109464711406\n",
            "7. Feature DE_SOLAR: Importance = 0.04139701336029036\n",
            "8. Feature DE_WINDPOW: Importance = 0.041050594816436616\n",
            "9. Feature DE_NET_IMPORT: Importance = 0.040360876573712484\n",
            "10. Feature DE_RESIDUAL_LOAD: Importance = 0.03994235202017012\n",
            "11. Feature DE_FR_EXCHANGE: Importance = 0.03781127775691077\n",
            "12. Feature DE_CONSUMPTION: Importance = 0.035131270781504735\n",
            "13. Feature DE_LIGNITE: Importance = 0.034897597068050214\n",
            "14. Feature DE_GAS: Importance = 0.031608936102971524\n",
            "15. Feature DE_WIND: Importance = 0.03049036142193737\n",
            "16. Feature FR_WIND: Importance = 0.027894754090968556\n",
            "17. Feature DAY_ID: Importance = 0.027475593417851878\n",
            "18. Feature FR_SOLAR: Importance = 0.02611501331462095\n",
            "19. Feature DE_HYDRO: Importance = 0.024573528599703395\n",
            "20. Feature DE_NUCLEAR: Importance = 0.02441633613383662\n",
            "21. Feature DE_COAL: Importance = 0.024405275289556834\n",
            "22. Feature FR_CONSUMPTION: Importance = 0.02404243273546051\n",
            "23. Feature FR_TEMP: Importance = 0.02364440011687333\n",
            "24. Feature FR_COAL: Importance = 0.023023221503555152\n",
            "25. Feature FR_NET_IMPORT: Importance = 0.022732212086470114\n",
            "26. Feature DE_RAIN: Importance = 0.021000132797703554\n",
            "27. Feature FR_HYDRO: Importance = 0.020770940383447338\n",
            "28. Feature COAL_RET: Importance = 0.017620850267654545\n",
            "29. Feature FR_RAIN: Importance = 0.01754434075701454\n",
            "30. Feature FR_NUCLEAR: Importance = 0.01732059992171487\n",
            "31. Feature DE_TEMP: Importance = 0.015527688431354016\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we can see that the most important features are the country, the gas price and the french windpower.\n",
        "Here, we calculate the most important features based on the impurity reduction. Features that lead to greater impurity reduction when used in the tree nodes are considered more important.\n",
        "\n"
      ],
      "metadata": {
        "id": "3AP07V4g-xhd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "result = permutation_importance(\n",
        "    model, X_val, Y_val.drop(columns=['ID']).values.ravel(), n_repeats=30, random_state=42\n",
        ")\n",
        "\n",
        "#Print feature importances\n",
        "feature_importance = result.importances_mean\n",
        "for feature, importance in zip(X_val.columns, feature_importance):\n",
        "    print(f'{feature}:{importance}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcxUNkSLHuSj",
        "outputId": "077befc4-c84b-4ba8-fd98-67dcf1a1b682"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DAY_ID:-0.003948870445356686\n",
            "DE_CONSUMPTION:-0.013795871448043565\n",
            "FR_CONSUMPTION:-0.009045379708466564\n",
            "DE_FR_EXCHANGE:-0.0022843741290210348\n",
            "DE_NET_IMPORT:0.00190005036076261\n",
            "FR_NET_IMPORT:-0.007387389701128932\n",
            "DE_GAS:-0.009495720814810812\n",
            "FR_GAS:-0.024484363344200566\n",
            "DE_COAL:-0.0053515438069910255\n",
            "FR_COAL:-0.0035897145494320883\n",
            "DE_HYDRO:-0.004036565751573754\n",
            "FR_HYDRO:-0.007062632081786865\n",
            "DE_NUCLEAR:-0.01157822044447423\n",
            "FR_NUCLEAR:-0.0030006513504041415\n",
            "DE_SOLAR:-0.006081569693745573\n",
            "FR_SOLAR:-0.00477751836129896\n",
            "DE_WINDPOW:0.0011824340665952172\n",
            "FR_WINDPOW:-0.007857326093125358\n",
            "DE_LIGNITE:-0.011493258487166721\n",
            "DE_RESIDUAL_LOAD:0.008649829427971767\n",
            "DE_RAIN:-0.0017245262984826389\n",
            "FR_RAIN:-0.003034705192578861\n",
            "DE_WIND:-0.0033043604666919803\n",
            "FR_WIND:-0.00392872646005881\n",
            "DE_TEMP:-0.0033792531841365585\n",
            "FR_TEMP:-0.0038913553384845034\n",
            "GAS_RET:-0.009462270173136056\n",
            "COAL_RET:-0.007707958989795409\n",
            "CARBON_RET:-0.008673047214353564\n",
            "COUNTRY_DE:0.002655419508387339\n",
            "COUNTRY_FR:0.006827140981687389\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XgBoost"
      ],
      "metadata": {
        "id": "8FEjw2zpKh7l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train_clean, Y_train_clean, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the XGBoost model with regularization parameters\n",
        "model = xgb.XGBRegressor(\n",
        "    max_depth=2,       # Maximum tree depth\n",
        "    min_child_weight=1,   # Minimum sum of instance weight needed in a child\n",
        "    gamma=0,           # Minimum loss reduction required to make a further partition on a leaf node\n",
        "    subsample=0.8,     # Subsample ratio of the training instances\n",
        "    colsample_bytree=0.8, # Subsample ratio of columns when constructing each tree\n",
        "    reg_alpha=0,       # L1 regularization term on weights\n",
        "    reg_lambda=1       # L2 regularization term on weights\n",
        ")\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "# Make predictions on the validation data\n",
        "output_val = model.predict(X_val)\n",
        "\n",
        "# Calculate the custom evaluation metric on the validation data\n",
        "spearman_correlation = metric_train(Y_val, output_val)\n",
        "print(\"Spearman correlation score on validation set:\", spearman_correlation)\n",
        "\n",
        "mse = mean_squared_error(Y_val, output_val)\n",
        "print(\"MSE:\",mse)\n",
        "mae = mean_absolute_error(Y_train_clean, output_train)\n",
        "print(\"MAE:\",mae)\n",
        "\n",
        "# Calculate Normalized Mean Absolute Error (NMAE)\n",
        "range_y_val = np.max(Y_val) - np.min(Y_val)\n",
        "nmae = mae / range_y_val\n",
        "print(\"NMAE:\", nmae)\n",
        "\n",
        "# Calculate Normalized Mean Squared Error (NMSE)\n",
        "var_y_val = np.var(Y_val)\n",
        "nmse = mse / var_y_val\n",
        "print(\"NMSE:\", nmse)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBrzWCKxmtwn",
        "outputId": "c9b8dfb0-a4fa-4d81-ddf3-1ece2d58e5af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman correlation score on validation set: 0.1338286458216426\n",
            "MSE: 1.3246923780632476\n",
            "MAE: 0.08839304488357316\n",
            "NMAE: 0.00777388338448308\n",
            "NMSE: 1.1091115390106603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jgtQpfQHKbAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AdaBoost"
      ],
      "metadata": {
        "id": "GBT9aVIKKbnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, Y_train, Y_val = train_test_split(X_train_clean, Y_train_clean, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the AdaBoost model\n",
        "base_estimator = DecisionTreeRegressor(max_depth=2)  # Base estimator for AdaBoost\n",
        "n_estimators = 100  # Number of boosting stages\n",
        "model = AdaBoostRegressor(base_estimator=base_estimator, n_estimators=n_estimators, random_state=42)\n",
        "\n",
        "# Fit the model on the training data\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "# Make predictions on the validation data\n",
        "output_val = model.predict(X_val)\n",
        "\n",
        "# Calculate the custom evaluation metric on the validation data\n",
        "spearman_correlation = metric_train(Y_val, output_val)\n",
        "print(\"Spearman correlation score on validation set:\", spearman_correlation)\n",
        "\n",
        "# Calculate mean squared error and mean absolute error\n",
        "mse = mean_squared_error(Y_val, output_val)\n",
        "print(\"MSE:\", mse)\n",
        "mae = mean_absolute_error(Y_val, output_val)\n",
        "print(\"MAE:\", mae)\n",
        "\n",
        "# Calculate Normalized Mean Absolute Error (NMAE)\n",
        "range_y_val = np.max(Y_val) - np.min(Y_val)\n",
        "nmae = mae / range_y_val\n",
        "print(\"NMAE:\", nmae)\n",
        "\n",
        "# Calculate Normalized Mean Squared Error (NMSE)\n",
        "var_y_val = np.var(Y_val)\n",
        "nmse = mse / var_y_val\n",
        "print(\"NMSE:\", nmse)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gN4Vx0C_8srS",
        "outputId": "3c7fc8a3-9538-4ff9-a8bc-bace4dec108d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_base.py:166: FutureWarning: `base_estimator` was renamed to `estimator` in version 1.2 and will be removed in 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spearman correlation score on validation set: 0.13447891650040628\n",
            "MSE: 1.694381906694985\n",
            "MAE: 0.9855346622064585\n",
            "NMAE: 0.0866745969148385\n",
            "NMSE: 1.4186376817188622\n"
          ]
        }
      ]
    }
  ]
}